{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requerimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/transformers\n",
    "#pip install transformers==2.5.0    \n",
    "#conda install -c anaconda tensorflow-gpu\n",
    "#conda install -c conda-forge/label/cf202003 ftfy\n",
    "#conda install -c anaconda gensim\n",
    "#conda install -c conda-forge/label/cf202003 spacy-model-es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model name 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-japanese, bert-base-japanese-whole-word-masking, bert-base-japanese-char, bert-base-japanese-char-whole-word-masking, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased, openai-gpt, transfo-xl-wt103, gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2, ctrl, xlnet-base-cased, xlnet-large-cased, xlm-mlm-en-2048, xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024, xlm-mlm-17-1280, xlm-mlm-100-1280, roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector, distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased, distilbert-base-uncased-finetuned-sst-2-english, albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2, camembert-base, umberto-commoncrawl-cased-v1, umberto-wikipedia-uncased-v1, t5-small, t5-base, t5-large, t5-3b, t5-11b, xlm-roberta-base, xlm-roberta-large, xlm-roberta-large-finetuned-conll02-dutch, xlm-roberta-large-finetuned-conll02-spanish, xlm-roberta-large-finetuned-conll03-english, xlm-roberta-large-finetuned-conll03-german, flaubert-small-cased, flaubert-base-uncased, flaubert-base-cased, flaubert-large-cased). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/modelcard.json' was a path or url to a model card file named modelcard.json or a directory containing such a file but couldn't find any such file at this path or url.\n",
      "Creating an empty model card.\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\n",
    "    'question-answering', \n",
    "    model='mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
    "    tokenizer=(\n",
    "        'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',  \n",
    "        {\"use_fast\": False}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establecer contexto y hacer preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "add example index and unique id: 100%|███████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.7057718707267497,\n",
       " 'start': 0,\n",
       " 'end': 15,\n",
       " 'answer': 'Mateo Guerrero'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = 'Mateo Guerrero  está trabajando en la universidad de Nariño en la materia electiva de base de datos'\n",
    "\n",
    "\n",
    "nlp(\n",
    "    {\n",
    "        'question': '¿Quién está trabajando  en la universidad de Nariño?',\n",
    "        'context': context\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "add example index and unique id: 100%|███████████████████████████████████████████████████████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.27831726435800874,\n",
       " 'start': 86,\n",
       " 'end': 98,\n",
       " 'answer': 'base de datos'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\n",
    "    {\n",
    "        'question': '¿En que materia?',\n",
    "        'context': context\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importar modelo análisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model name 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-japanese, bert-base-japanese-whole-word-masking, bert-base-japanese-char, bert-base-japanese-char-whole-word-masking, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased, openai-gpt, transfo-xl-wt103, gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2, ctrl, xlnet-base-cased, xlnet-large-cased, xlm-mlm-en-2048, xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024, xlm-mlm-17-1280, xlm-mlm-100-1280, roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector, distilbert-base-uncased, distilbert-base-uncased-distilled-squad, distilbert-base-cased, distilbert-base-cased-distilled-squad, distilbert-base-german-cased, distilbert-base-multilingual-cased, distilbert-base-uncased-finetuned-sst-2-english, albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2, camembert-base, umberto-commoncrawl-cased-v1, umberto-wikipedia-uncased-v1, t5-small, t5-base, t5-large, t5-3b, t5-11b, xlm-roberta-base, xlm-roberta-large, xlm-roberta-large-finetuned-conll02-dutch, xlm-roberta-large-finetuned-conll02-spanish, xlm-roberta-large-finetuned-conll03-english, xlm-roberta-large-finetuned-conll03-german, flaubert-small-cased, flaubert-base-uncased, flaubert-base-cased, flaubert-large-cased). We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es/modelcard.json' was a path or url to a model card file named modelcard.json or a directory containing such a file but couldn't find any such file at this path or url.\n",
      "Creating an empty model card.\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\n",
    "    'sentiment-analysis', \n",
    "    model='mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
    "    tokenizer=(\n",
    "        'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',  \n",
    "        {\"use_fast\": False}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.5002289}]\n",
      "[{'label': 'LABEL_0', 'score': 0.5648577}]\n"
     ]
    }
   ],
   "source": [
    "print(nlp('estoy feliz y contento de estar aquí en clases que alegría'))\n",
    "print(nlp('gobierno opresor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wor2vec\n",
    "\n",
    "Una aplicación importante del aprendizaje automático en el análisis de texto, el algoritmo Word2Vec es una herramienta fascinante y muy útil. Como sugiere el nombre, crea una representación vectorial de palabras basada en el corpus que estamos usando.\n",
    "\n",
    "Pero la magia de Word2Vec está en cómo logra capturar la representación semántica de palabras en un vector [Mikolov 2013].\n",
    "\n",
    "Instalar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from time import time  # To time our operations\n",
    "import multiprocessing\n",
    "from ftfy import fix_encoding\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'En este trabajo se presenta el análisis, diseño e implementación de TariyKDD,una herramienta genérica para el Descubrimiento de conocimiento en bases de datos, débilmente acoplada con el SGBD PostgreSQL. TariyKDD comprende cuatro módulos que cubren la conexión, a archivos planos y bases de datos relacionales, un módulo de utilidades con clases y librerías comunes, un módulo kernel que reune las etapas de preprocesamiento, minería y visualización, y el módulo de interfaz gráfica de usuario. Dentro del kernel de minería se implementan 5 algoritmos, Apriori, FPGrowth y EquipAsso para asociación y C4.5 y Mate para clasificación. Se evalúa el rendimiento de los algoritmos EquipAsso, un algoritmo para el cálculo de conjuntos de ítems frecuentes, y Mate, un algoritmo para la construcción de arboles de clasificación, basados en nuevos operadores del álgebra relacional, con respecto a los algoritmos Apriori y FP-Growth y C4.5, respectivamente. 1. INTRODUCCION El proceso de extraer conocimiento a partir de grandes volúmenes de datos ha sido reconocido por muchos investigadores como un tópico de investigación clave en los sistemas de bases de datos, y por muchas compañías industriales como una importante área y una oportunidad para obtener mayores ganancias [42]. El Descubrimiento de Conocimiento en Bases de Datos (DCBD) es básicamente un proceso automático en el que se combinan descubrimiento y análisis. El proceso consiste en extraer patrones en forma de reglas o funciones, a partir de los datos, para que el usuario los analice. Esta tarea implica generalmente preprocesar los datos, hacer minería de datos (data mining) y presentar resultados [3, 2, 7, 23, 31]. El DCBD se puede aplicar en diferentes dominios por ejemplo, para determinar perfiles de clientes fraudulentos (evasión de impuestos), para descubrir relaciones implícitas existentes entre síntomas y enfermedades, entre características técnicas y diagnóstico del estado de equipos y máquinas, para determinar perfiles de estudiantes ”académicamente exitosos” en términos de sus características socioeconómicas, para determinar patrones de compra de los clientes en sus canastas de mercado, entre otras. Las investigaciones en DCBD, se centraron inicialmente en definir nuevas operaciones de descubrimiento de patrones y desarrollar algoritmos para estas. Investigaciones posteriores se han focalizado en el problema de integrar DCBD con Sistemas Gestores de Bases de Datos (SGBD) ofreciendo como resultado el desarrollo de herramientas DCBD cuyas arquitecturas se pueden clasificar en una de tres categorías: débilmente acopladas, medianamente acopladas y fuertemente acopladas con el SGBD [41]. Una herramienta DCBD debe integrar una variedad de componentes (técnicas de minería de datos, consultas, métodos de visualización, interfaces, etc.), que juntos puedan eficientemente identificar y extraer patrones interesantes y útiles de los datos almacenados en las bases de datos. De acuerdo a las tareas que desarrollen, las herramientas DCBD se clasifican en tres grupos: herramientas genéricas de tareas sencillas, herramientas genéricas de tareas múltiples y herramientas de dominio específico [31]. En este documento se presenta el trabajo de grado para optar por el título de Ingeniero de Sistemas. Fruto de la presente investigación es el desarrollo de ”TariyKDD: Una herramienta genérica de Descubrimiento de Conocimiento en Bases de Datos débilmente acoplada con el SGBD PostgreSQL”, en la cual también se implementaron los algoritmos EquipAsso [43, 42, 45] y MateTree [46] para las tareas de Asociación y Clasificación propuestos por Timarán en [46] y sobre los cuales se realizarón ciertas pruebas para medir su rendimiento. El resto de este documento esta organizado de la siguiente manera. En la primera sección se especifica el tema de la propuesta, se lo enmarca dentro de una línea de investigación y se lo delimita. A 16 continuación se describe el problema objeto de estudio, luego se especifican los objetivos generales y específicos del proyecto y finalmente la justificación. En la sección 2 se presenta el estado general del arte en el área de integración de DCBD y SGBD. En la sección 3 se desarrolla todo lo concerniente al análisis orientado a objetos UML que se realizo para construir la herramienta, en la sección 4 se presenta la implementación del proyecto, en la sección 5 se presenta las pruebas hechas y resultados, y finalmente en la sección 6 se presenta las conclusiones de este trabajo. 6. CONCLUSIONES Y TRABAJOS FUTUROS En este proyecto se diseñó e implementó una herramienta débilmente acoplada con el SGBD PostgreSQL que da soporte a las etapas de conexión, preprocesamiento, minería y visualización del proceso KDD. Igualmente se incluyeron en el estudio nuevos algoritmos de asociación y clasificación propuestos por Timaran [42,43,44,45]. Para el desarrollo del proyecto se hizo un análisis de varias herramientas de software libre que abordan tareas similares a las que se pretendió en este trabajo. Se identificó las limitaciones y virtudes de estas aplicaciones y se diseñó una metodología para el desarrollo de una herramienta que cubriera las falencias encontradas. Se incluyó en TariyKDD mejoras en la conexión a bases de datos implementando un interfaz que permite la selección visual de atributos y el establecimiento de relaciones entre diferentes tablas pertenecientes a una misma base de datos. Se implementó nuevos algoritmos de asociación y clasificación ausentes en otras herramientas como FPGrowth, EquipAsso y Mate. Igualmente se construyó visores que permiten la interacción directa con los resultados obtenidos a través de tablas ordenadas, arboles jerarquizados y gráficos de distribución. Se establecieron patrones de diseño que hicieron posible el acoplamiento de nuevas funcionalidades a cada uno de los módulos que lo componen, facilitando así la inclusión futura de nuevas características y el mejoramiento continuo de la aplicación, así como la implementación de un sistema de control de versiones que facilita el trabajo colaborativo y permite el acceso a la herramienta a través de Internet. La construcción de TariyKDD comprendió el desarrollo de cuatro módulos que cubrieron: El módulo de conexión a datos, tanto a archivos planos como a bases de datos relacionales. El módulo de utilidades, que contiene una colección de clases y librerías comunes usadas en toda la aplicación. El módulo de kernel, que incluye la etapas de: preprocesamiento, donde se implementaron 9 filtros para la selección, transformación y preparación de los datos, el proceso de minería, que comprendió tareas de asociación y clasificación, implementando 5 algoritmos, Apriori, FPGrowth y EquipAsso para asociación y C4.5 y Mate para clasificación y el proceso de visualización de resultados, utilizando tablas y árboles para generar reportes de los resultados y reglas obtenidas. Por último, el módulo de interfaz gráfica de usuario que provee una interacción amigable sobre los demás componentes por parte del usuario. 201 Se incluyó un módulo para la predicción de nuevos registros a partir de los modelos construidos con los algoritmos de clasificación. Se desarrolló un modelo de datos que facilitó la aplicación de algoritmos de asociación sobre bases de datos enmarcadas en el concepto de canasta de mercado donde la longitud de cada transacción es variable. El desarrollo de TariyKDD fue logrado usando en su totalidad herramientas de código abierto y software libre. Se realizaron pruebas para evaluar la validez de los algoritmos implementados. Para el plan de pruebas de la tarea de asociación, se utilizaron conjuntos de datos reales de las ventas de un supermercado de la Caja de Compensación Familiar de Nariño compuesta por 85.692 transacciones. Para Clasificación, se trabajó con la base de datos histórica de los estudiantes de la Universidad de Nariño, compuesta por información personal y académica de 20.328 estudiantes. Como resultado de las pruebas para la tarea de clasificación se concluye que es recomendable el uso del algoritmo C4.5 para conjuntos de datos extensos donde el número de atributos a analizar sea grande. Solo bajo conjuntos de datos pequeños y con reducido número de atributos resulta viable aplicar el algoritmo Mate bajo una arquitectura débilmente acoplada. Como resultado de las pruebas para Asociación se concluye que los algoritmos FPGrowth y EquipAsso obtienen buenos tiempos de respuesta al ser aplicados en conjuntos grandes pero a medida que se disminuye el criterio de soporte se ve un mejor comportamiento por parte del algoritmo EquipAsso. No es viable aplicar el algoritmo Apriori bajo conjuntos grandes limitando su uso a muestras pequeñas de datos. Este proyecto complementa el trabajo “Proyecto TARIY – Un Análisis de Rendimiento de Algoritmos para Reglas de Asociación” [36] que fue ganador de la VII Convocatoria Alberto Quijano Guerrero de proyectos de investigación estudiantil en la categoría de Investigación Cuantitativa presentado por los mismos autores. Este proyecto hizo parte de los proyectos financiados por el Sistema de Investigaciones de la Universidad de Nariño dentro del marco del Concurso de Tesis de Pregrado organizado en el año 2006. Durante el desarrollo del proyecto se participó activamente en eventos regionales como el Día Internacional del Software Libre y la Semana de Ingeniería. Como resultado de este proyecto se publicó y presentó un artículo internacional en el marco del XXXII Congreso Latinoamericano de Estudios Informáticos - CLEI 2006 realizado en la ciudad de Santiago de Chile [40]. 202 Se cuenta con una versión estable de TariyKDD con la capacidad de extraer reglas asociación y clasificación bajo una arquitectura débilmente acoplada con el SGBD PostgreSQL desarrollada bajo los lineamientos del software libre. Una vez que se han descrito los resultados más relevantes que se han obtenido durante la realización de este proyecto, se sugiere una serie de recomendaciones como punto de partida para trabajos futuros: • Realizar mayores pruebas de rendimiento de esta arquitectura con otros repositorios de datos reales. • Implementar otras primitivas que Timaran propone para tareas de Asociación y Clasificación. • Implementar otras tareas y algoritmos de minería de datos, como clustering y patrones secuenciales. • Implementar nuevos filtros e interfaces de visualización que permitan el mejoramiento continuo de TariyKDD. • Acoplar gráficos estadísticos a los conjuntos de datos cargados para obtener una información inicial de sus características. • Acoplar TariyKDD fuertemente con PostgreKDD. • Disponer de la herramienta como material de apoyo a las electivas de base de datos dentro del programa de Ingeniería de Sistemas. • Liberar, compartir y difundir una versión estable de TariyKDD con la capacidad de descubrir conocimiento en bases de datos. • Asegurar la continuidad del proyecto. Finalmente, este trabajo permitió aplicar los conocimientos adquiridos en el programa de Ingeniería de Sistemas y en especial los de la electiva de bases de datos, así como el trabajo y aprendizaje dentro del Grupo de Investigación GRiAS Línea KDD. '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "ruta_archivo=\"./texto_prueba.txt\"\n",
    "f = open (ruta_archivo,'r',encoding=\"utf8\")\n",
    "texto = f.read()\n",
    "f.close()\n",
    "texto = re.sub('[ \\t\\n]+', ' ', texto) \n",
    "texto=fix_encoding(texto)\n",
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear Función para tokenizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['desarrollar',\n",
       " 'sistema',\n",
       " 'información',\n",
       " 'administrar',\n",
       " 'dato',\n",
       " 'bienestar',\n",
       " 'familiar']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    doc1 = nlp(text)\n",
    "    words=[]\n",
    "    for t in doc1:\n",
    "        if  t.is_punct or  t.is_stop or t.is_space or len(str(t))<=3:\n",
    "            continue\n",
    "        if t.ent_type==0:\n",
    "            words.append(str(t.lemma_).lower())\n",
    "        else:\n",
    "            words.append(str(t).lower())\n",
    "    return words\n",
    "\n",
    "tokenize(\"desarrollo de un sistema de información que administre los datos de bienestar familiar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['presentar', 'análisis', 'diseñar', 'implementación', 'tariykdd', 'herramienta', 'genérico', 'descubrimiento', 'conocimiento', 'base', 'dato', 'débilmente', 'acoplar', 'sgbd', 'postgresql'], ['tariykdd', 'comprender', 'módulo', 'cubrir', 'conexión', 'archivo', 'plano', 'base', 'dato', 'relacional', 'módulo', 'utilidad', 'clase', 'librería', 'común', 'módulo', 'kernel', 'reune', 'etapa', 'preprocesamiento', 'minería', 'visualización', 'módulo', 'interfaz', 'gráfico', 'usuario'], ['kernel', 'minería', 'implementar', 'algoritmo', 'apriori', 'fpgrowth', 'equipasso', 'asociación', 'c4.5', 'mate', 'clasificación'], ['evaluar', 'rendimiento', 'algoritmo', 'equipasso', 'algoritmo', 'cálculo', 'conjunto', 'ítem', 'frecuente', 'mate', 'algoritmo', 'construcción', 'arbolar', 'clasificación', 'basar', 'operador', 'álgebra', 'relacional', 'algoritmo', 'apriori', 'growth', 'c4.5', 'respectivamente'], [], ['introduccion', 'procesar', 'extraer', 'conocimiento', 'volumen', 'dato', 'reconocer', 'investigador', 'tópico', 'investigación', 'clavar', 'sistema', 'base', 'dato', 'compañía', 'industrial', 'importante', 'área', 'oportunidad', 'obtener', 'mayor', 'ganancia'], ['descubrimiento', 'conocimiento', 'bases', 'datos', 'dcbd', 'básicamente', 'procesar', 'automático', 'combinar', 'descubrimiento', 'análisis'], ['procesar', 'consistir', 'extraer', 'patrón', 'formar', 'reglar', 'funcionar', 'dato', 'usuario', 'analizar'], ['tarea', 'implicar', 'generalmente', 'preprocesar', 'dato', 'minería', 'dato', 'datar', 'mining', 'presentar', 'resultar'], ['dcbd', 'aplicar', 'dominio', 'determinar', 'perfilar', 'cliente', 'fraudulento', 'evasión', 'impuesto', 'descubrir', 'relacionar', 'implícito', 'existente', 'síntoma', 'enfermedad', 'característico', 'técnico', 'diagnóstico', 'equipo', 'máquina', 'determinar', 'perfilar', 'estudiante', 'académicamente', 'exitoso', 'término', 'característico', 'socioeconómico', 'determinar', 'patrón', 'comprar', 'cliente', 'canasta', 'mercar'], ['investigación', 'dcbd', 'centrar', 'inicialmente', 'definir', 'operación', 'descubrimiento', 'patrón', 'desarrollar', 'algoritmo'], ['investigaciones', 'posterior', 'focalizar', 'problema', 'integrar', 'dcbd', 'sistemas', 'gestores', 'bases', 'datos', 'sgbd', 'ofrecer', 'resultar', 'desarrollar', 'herramienta', 'dcbd', 'cuyo', 'arquitectura', 'clasificar', 'categoría', 'débilmente', 'acoplar', 'medianamente', 'acoplar', 'fuertemente', 'acoplar', 'sgbd'], ['herramienta', 'dcbd', 'integrar', 'variedad', 'componente', 'técnico', 'minería', 'dato', 'consultar', 'método', 'visualización', 'interfaz', 'etcétera', 'junto', 'poder', 'eficientemente', 'identificar', 'extraer', 'patrón', 'interesante', 'útil', 'dato', 'almacenar', 'base', 'dato'], ['tarea', 'desarrollar', 'herramienta', 'dcbd', 'clasificar', 'grupo', 'herramienta', 'genérico', 'tarea', 'sencillo', 'herramienta', 'genérico', 'tarea', 'múltiple', 'herramienta', 'dominio', 'específico'], ['documentar', 'presentar', 'gradar', 'optar', 'título', 'ingeniero', 'sistemas'], ['fruto', 'presentar', 'investigación', 'desarrollar', 'tariykdd', 'herramienta', 'genérica', 'descubrimiento', 'conocimiento', 'bases', 'datos', 'débilmente', 'acoplar', 'sgbd', 'postgresql', 'implementar', 'algoritmo', 'equipasso', 'matetree', 'tarea', 'asociación', 'clasificación', 'proponer', 'timarán', 'realizarón', 'prueba', 'medir', 'rendimiento'], ['restar', 'documentar', 'organizar'], ['sección', 'especificar', 'temer', 'proponer', 'enmarcar', 'líneo', 'investigación', 'delimitar'], ['continuación', 'describir', 'problema', 'objetar', 'estudiar', 'especificar', 'objetivo', 'general', 'específico', 'proyectar', 'finalmente', 'justificación'], ['sección', 'presentar', 'arte', 'área', 'integración', 'dcbd', 'sgbd'], ['sección', 'desarrollar', 'concerniente', 'análisis', 'orientar', 'objeto', 'realizar', 'construir', 'herramienta', 'sección', 'presentar', 'implementación', 'proyectar', 'sección', 'presentar', 'prueba', 'hacer', 'resultar', 'finalmente', 'sección', 'presentar', 'conclusión'], [], ['conclusiones', 'trabajos', 'futuros', 'proyectar', 'diseñar', 'implementar', 'herramienta', 'débilmente', 'acoplar', 'sgbd', 'postgresql', 'soportar', 'etapa', 'conexión', 'preprocesamiento', 'minería', 'visualización', 'procesar'], ['igualmente', 'incluir', 'estudiar', 'algoritmo', 'asociación', 'clasificación', 'proponer', 'timaran', '42,43,44,45'], ['desarrollar', 'proyectar', 'análisis', 'herramienta', 'software', 'librar', 'abordar', 'tarea', 'similar', 'pretender'], ['identificar', 'limitación', 'virtud', 'aplicación', 'diseñar', 'metodología', 'desarrollar', 'herramienta', 'cubrir', 'falencias', 'encontrar'], ['incluir', 'tariykdd', 'mejorar', 'conexión', 'base', 'dato', 'implementar', 'interfaz', 'permitir', 'selección', 'visual', 'atributo', 'establecimiento', 'relacionar', 'tabla', 'perteneciente', 'basar', 'dato'], ['implementar', 'algoritmo', 'asociación', 'clasificación', 'ausente', 'herramienta', 'fpgrowth', 'equipasso', 'mate'], ['igualmente', 'construir', 'visor', 'permitir', 'interacción', 'directo', 'resultar', 'obtener', 'tabla', 'ordenar', 'arbolar', 'jerarquizar', 'gráfico', 'distribución'], ['establecer', 'patrón', 'diseñar', 'acoplamiento', 'funcionalidad', 'módulo', 'componer', 'facilitar', 'inclusión', 'futuro', 'característico', 'mejoramiento', 'continuo', 'aplicación', 'implementación', 'sistema', 'control', 'versionar', 'facilitar', 'colaborativo', 'permitir', 'acceso', 'herramienta', 'internet'], ['construcción', 'tariykdd', 'comprender', 'desarrollar', 'módulo', 'cubrir', 'módulo', 'conexión', 'dato', 'archivo', 'plano', 'base', 'dato', 'relacional'], ['módulo', 'utilidad', 'contener', 'colección', 'clase', 'librería', 'común', 'usar', 'aplicación'], ['módulo', 'kernel', 'incluir', 'etapa', 'preprocesamiento', 'implementar', 'filtro', 'selección', 'transformación', 'preparación', 'dato', 'procesar', 'minería', 'comprender', 'tarea', 'asociación', 'clasificación', 'implementar', 'algoritmo', 'apriori', 'fpgrowth', 'equipasso', 'asociación', 'c4.5', 'mate', 'clasificación', 'procesar', 'visualización', 'resultar', 'utilizar', 'tabla', 'árbol', 'generar', 'reporte', 'resultar', 'reglar', 'obtener'], ['módulo', 'interfaz', 'gráfico', 'usuario', 'proveer', 'interacción', 'amigable', 'componente', 'usuario'], ['incluir', 'módulo', 'predicción', 'registro', 'modelo', 'construir', 'algoritmo', 'clasificación'], ['desarrollar', 'modelar', 'dato', 'facilitar', 'aplicación', 'algoritmo', 'asociación', 'base', 'dato', 'enmarcar', 'concepto', 'canasta', 'mercar', 'longitud', 'transacción', 'variable'], ['desarrollar', 'tariykdd', 'lograr', 'usar', 'totalidad', 'herramienta', 'código', 'abrir', 'software', 'librar'], ['realizar', 'prueba', 'evaluar', 'validez', 'algoritmo', 'implementar'], ['plan', 'prueba', 'tarea', 'asociación', 'utilizar', 'conjunto', 'dato', 'real', 'venta', 'supermercado', 'caja', 'compensación', 'familiar', 'nariño', 'componer', '85.692', 'transaccionar'], ['clasificación', 'trabajar', 'basar', 'dato', 'histórico', 'estudiante', 'universidad', 'nariño', 'componer', 'información', 'personal', 'académico', '20.328', 'estudiante'], ['resultar', 'prueba', 'tarea', 'clasificación', 'concluir', 'recomendable', 'algoritmo', 'c4.5', 'conjunto', 'dato', 'extenso', 'número', 'atributo', 'analizar', 'grande'], ['conjunto', 'dato', 'pequeño', 'reducir', 'número', 'atributo', 'resultar', 'viable', 'aplicar', 'algoritmo', 'mate', 'arquitectura', 'débilmente', 'acoplar'], ['resultar', 'prueba', 'asociación', 'concluir', 'algoritmo', 'fpgrowth', 'equipasso', 'obtener', 'tiempo', 'respuesta', 'aplicar', 'conjunto', 'medir', 'disminuir', 'criterio', 'soportar', 'comportamiento', 'algoritmo', 'equipasso'], ['viable', 'aplicar', 'algoritmo', 'apriori', 'conjunto', 'limitar', 'muestra', 'pequeño', 'dato'], ['proyectar', 'complementar', 'proyecto', 'tariy', 'análisis', 'rendimiento', 'algoritmos', 'reglas', 'asociación', 'ganador', 'convocatoria', 'alberto', 'quijano', 'guerrero', 'proyecto', 'investigación', 'estudiantil', 'categoría', 'investigación', 'cuantitativa', 'presentar', 'autor'], ['proyectar', 'proyecto', 'financiar', 'sistema', 'investigaciones', 'universidad', 'nariño', 'marcar', 'concurso', 'tesis', 'pregrado', 'organizar', '2006'], ['desarrollar', 'proyectar', 'participar', 'activamente', 'evento', 'regional', 'internacional', 'software', 'libre', 'semana', 'ingeniería'], ['resultar', 'proyectar', 'publicar', 'presentar', 'artículo', 'internacional', 'marcar', 'xxxii', 'congreso', 'latinoamericano', 'estudios', 'informáticos', 'clei', '2006', 'ciudad', 'santiago', 'chile'], ['versión', 'estable', 'tariykdd', 'capacidad', 'extraer', 'reglar', 'asociación', 'clasificación', 'arquitectura', 'débilmente', 'acoplar', 'sgbd', 'postgresql', 'desarrollar', 'lineamiento', 'software', 'librar'], ['describir', 'resultar', 'relevante', 'obtener', 'realización', 'proyectar', 'sugerir', 'seriar', 'recomendación', 'punto', 'partir', 'trabajo', 'futuro', 'mayor', 'prueba', 'rendimiento', 'arquitectura', 'repositorio', 'dato', 'real'], ['implementar', 'primitivo', 'timaran', 'proponer', 'tarea', 'asociación', 'clasificación'], ['implementar', 'tarea', 'algoritmo', 'minería', 'dato', 'clustering', 'patrón', 'secuencial'], ['implementar', 'filtro', 'interfaz', 'visualización', 'permitir', 'mejoramiento', 'continuo', 'tariykdd'], ['acoplar', 'gráfico', 'estadístico', 'conjunto', 'dato', 'cargar', 'obtener', 'información', 'inicial', 'característico'], ['acoplar', 'tariykdd', 'fuertemente', 'postgrekdd'], ['disponer', 'herramienta', 'material', 'apoyar', 'electivo', 'basar', 'dato', 'programar', 'ingeniería', 'sistemas'], ['liberar', 'compartir', 'difundir', 'versión', 'estable', 'tariykdd', 'capacidad', 'descubrir', 'conocimiento', 'base', 'dato'], ['asegurar', 'continuidad', 'proyectar'], ['finalmente', 'permitir', 'aplicar', 'conocimiento', 'adquirir', 'programar', 'ingeniería', 'sistemas', 'especial', 'electivo', 'base', 'dato', 'aprendizaje', 'grupo', 'investigación', 'grias', 'línea']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-04a90f37ae17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mmodelo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-04a90f37ae17>\u001b[0m in \u001b[0;36mentrenamiento\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# preparar el vocabulario modelo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tiempo para desarrollar vocabulario: {} minutos'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#entrenar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "def entrenamiento(texto):\n",
    "        doc=nlp(fix_encoding(texto))\n",
    "        sent=[]\n",
    "        #Recorrer oraciones del texto\n",
    "        for num,oracion in enumerate(doc.sents):\n",
    "            o=tokenize(str(oracion)) #tokenizar oración  \n",
    "            sent.append(o) \n",
    "        print(sent)\n",
    "        #Crea las frases relevantes de la lista de oraciones:\n",
    "        phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "        #Transforme el corpus en función de las bigramas detectadas:\n",
    "        bigram = Phraser(phrases)\n",
    "        sentences = bigram[sent]\n",
    "                  \n",
    "        #Entrenamiento del modelo\n",
    "        cores = multiprocessing.cpu_count() #cuenta el nro de nucles de la pc\n",
    "\n",
    "        w2v_model = Word2Vec(min_count=3,\n",
    "                     window=10,\n",
    "                     size=20,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-2\n",
    "                    )\n",
    "        t = time()\n",
    "        w2v_model.build_vocab(sentences,progress_per=10000)  # preparar el vocabulario modelo\n",
    "        print('Tiempo para desarrollar vocabulario: {} minutos'.format(round((time() - t) / 60, 2)))\n",
    "        t = time()\n",
    "        w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=10000, report_delay=1) #entrenar\n",
    "        print('Tiempo de entrenamiento del modelo: {} minutos'.format(round((time() - t) / 60, 2)))\n",
    "        return w2v_model\n",
    "        \n",
    "modelo=entrenamiento(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.wv.most_similar(positive=['algoritmo'.lower()],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.wv.most_similar(positive=['clasificación','asociación'],topn=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec\n",
    "\n",
    "Sobre la base de Word2Vec, los investigadores también han implementado una representación vectorial de documentos o párrafos, popularmente llamado Doc2Vec. Esto significa que ahora podemos usar el poder de la comprensión semántica de Word2Vec para describir documentos también, y en cualquier dimensión en la que nos gustaría entrenarlo!\n",
    "\n",
    "Construir nuestro modelo propio de análisis de sentimientos\n",
    "\n",
    "Importar librerías\n",
    "\n",
    "https://medium.com/@gruizdevilla/introducci%C3%B3n-a-word2vec-skip-gram-model-4800f72c871f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer el conjunto de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./tweet_sentimientos.csv\",sep=\"|\",index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['clase'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento y limpiez de los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clase</th>\n",
       "      <th>tweet_limpio</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abcdesevilla.es: Recio no tiene «indicios pote...</td>\n",
       "      <td>0</td>\n",
       "      <td>abcdesevilla es  recio no tiene indicios poten...</td>\n",
       "      <td>[abcdesevill, reci, indici, potent, denunci, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abcdesevilla.es: Cuatro altos cargos de Empleo...</td>\n",
       "      <td>0</td>\n",
       "      <td>abcdesevilla es  cuatro altos cargos de empleo...</td>\n",
       "      <td>[abcdesevill, altos, carg, denunci, retir, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La marcha atrás del PP en posponer devolución ...</td>\n",
       "      <td>0</td>\n",
       "      <td>la marcha atrás del pp en posponer devolución ...</td>\n",
       "      <td>[march, atras, pp, pospon, devolu, cca, resbal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accidente en BUS-VAO A-6 km. 12. Motorista de ...</td>\n",
       "      <td>0</td>\n",
       "      <td>accidente en bus vao a  km    motorista de  añ...</td>\n",
       "      <td>[accident, bus, vao, a, km, motor, años, her, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#FF a ti, que deseas desesperadamente hacerme ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ff a ti  que deseas desesperadamente hacerme ...</td>\n",
       "      <td>[ff, a, des, desesper, hac, ff, diran]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  clase  \\\n",
       "0  abcdesevilla.es: Recio no tiene «indicios pote...      0   \n",
       "1  abcdesevilla.es: Cuatro altos cargos de Empleo...      0   \n",
       "2  La marcha atrás del PP en posponer devolución ...      0   \n",
       "3  Accidente en BUS-VAO A-6 km. 12. Motorista de ...      0   \n",
       "4  #FF a ti, que deseas desesperadamente hacerme ...      0   \n",
       "\n",
       "                                        tweet_limpio  \\\n",
       "0  abcdesevilla es  recio no tiene indicios poten...   \n",
       "1  abcdesevilla es  cuatro altos cargos de empleo...   \n",
       "2  la marcha atrás del pp en posponer devolución ...   \n",
       "3  accidente en bus vao a  km    motorista de  añ...   \n",
       "4   ff a ti  que deseas desesperadamente hacerme ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [abcdesevill, reci, indici, potent, denunci, a...  \n",
       "1  [abcdesevill, altos, carg, denunci, retir, pre...  \n",
       "2  [march, atras, pp, pospon, devolu, cca, resbal...  \n",
       "3  [accident, bus, vao, a, km, motor, años, her, ...  \n",
       "4             [ff, a, des, desesper, hac, ff, diran]  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#Función Convertir el texto en minúsculas, eliminar el texto entre corchetes, eliminar la puntuación y eliminar las palabras que contienen números '' '.\n",
    "def texto_limpio(t):\n",
    "  t=str(t)\n",
    "  t=t.lower()\n",
    "  t=re.sub('\\[.*?¿\\]\\%', ' ', t)\n",
    "  t=re.sub('[%s]' % re.escape(string.punctuation), ' ', t)\n",
    "  t=re.sub('\\w*\\d\\w*', '', t)\n",
    "  t=re.sub('[‘’“”…«»]', '', t)\n",
    "  t=re.sub('\\n', ' ', t)\n",
    "  return t\n",
    "\n",
    "round = lambda x: texto_limpio(x)\n",
    "\n",
    "df['tweet_limpio'] = df.tweet.apply(round)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'administr'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "display(stemmer.stem('administrador'))\n",
    "\n",
    "def tokenizar_stem(text):\n",
    "    text=text.lower()\n",
    "    doc1 = nlp(text,disable = ['ner', 'parser'])\n",
    "    words=[]\n",
    "    for t in doc1:\n",
    "        if  t.is_punct or  t.is_stop or t.is_space:\n",
    "            continue\n",
    "        words.append(stemmer.stem(str(t)))\n",
    "\n",
    "    return words\n",
    "\n",
    "def tokenize_lema(text):\n",
    "    doc1 = nlp(text)\n",
    "    words=[]\n",
    "    for t in doc1:\n",
    "        if  t.is_punct or  t.is_stop or t.is_space or len(str(t))<=3:\n",
    "            continue\n",
    "        if t.ent_type==0:\n",
    "            words.append(str(t.lemma_).lower())\n",
    "        else:\n",
    "            words.append(str(t).lower())\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>clase</th>\n",
       "      <th>tweet_limpio</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abcdesevilla.es: Recio no tiene «indicios pote...</td>\n",
       "      <td>0</td>\n",
       "      <td>abcdesevilla es  recio no tiene indicios poten...</td>\n",
       "      <td>[abcdesevill, reci, indici, potent, denunci, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abcdesevilla.es: Cuatro altos cargos de Empleo...</td>\n",
       "      <td>0</td>\n",
       "      <td>abcdesevilla es  cuatro altos cargos de empleo...</td>\n",
       "      <td>[abcdesevill, altos, carg, denunci, retir, pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La marcha atrás del PP en posponer devolución ...</td>\n",
       "      <td>0</td>\n",
       "      <td>la marcha atrás del pp en posponer devolución ...</td>\n",
       "      <td>[march, atras, pp, pospon, devolu, cca, resbal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accidente en BUS-VAO A-6 km. 12. Motorista de ...</td>\n",
       "      <td>0</td>\n",
       "      <td>accidente en bus vao a  km    motorista de  añ...</td>\n",
       "      <td>[accident, bus, vao, a, km, motor, años, her, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#FF a ti, que deseas desesperadamente hacerme ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ff a ti  que deseas desesperadamente hacerme ...</td>\n",
       "      <td>[ff, a, des, desesper, hac, ff, diran]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  clase  \\\n",
       "0  abcdesevilla.es: Recio no tiene «indicios pote...      0   \n",
       "1  abcdesevilla.es: Cuatro altos cargos de Empleo...      0   \n",
       "2  La marcha atrás del PP en posponer devolución ...      0   \n",
       "3  Accidente en BUS-VAO A-6 km. 12. Motorista de ...      0   \n",
       "4  #FF a ti, que deseas desesperadamente hacerme ...      0   \n",
       "\n",
       "                                        tweet_limpio  \\\n",
       "0  abcdesevilla es  recio no tiene indicios poten...   \n",
       "1  abcdesevilla es  cuatro altos cargos de empleo...   \n",
       "2  la marcha atrás del pp en posponer devolución ...   \n",
       "3  accidente en bus vao a  km    motorista de  añ...   \n",
       "4   ff a ti  que deseas desesperadamente hacerme ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [abcdesevill, reci, indici, potent, denunci, a...  \n",
       "1  [abcdesevill, altos, carg, denunci, retir, pre...  \n",
       "2  [march, atras, pp, pospon, devolu, cca, resbal...  \n",
       "3  [accident, bus, vao, a, km, motor, años, her, ...  \n",
       "4             [ff, a, des, desesper, hac, ff, diran]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['tokens'] = df.apply(lambda row: tokenizar_stem(str(row['tweet_limpio'])),axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para construir nuestro corpus, usaremos la clase TaggedDocument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "        i=0\n",
    "        for line in df['tokens']:\n",
    "            tokens = gensim.utils.simple_preprocess(str(line))\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "            i=i+1\n",
    "train_corpus = list(read_corpus(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar modelo Doc2vec y guardarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m    811\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ruta_drive=\"./\"\n",
    "modelo=\"modelo_tweet\"\n",
    "\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(\n",
    "      vector_size=5, \n",
    "      min_count=1, \n",
    "      epochs=100,\n",
    "      hs=0, #Cero para negative sampling\n",
    "      negative=15, #si> 0, se utilizará un muestreo negativo, el int para negativo especifica cuántas \"palabras irrelevantes\" se deben dibujar (normalmente entre 5 y 20).\n",
    "      alpha=0.003, #Tasa de aprendizaje\n",
    "      min_alpha=0.00005, #Tasa que se reducira durante el train\n",
    "      workers=8,\n",
    "      dm=1,\n",
    "      sample=0.5,#El umbral para configurar qué palabras de mayor frecuencia se muestrean aleatoriamente, rango útil es (0, 1e-5).\n",
    "      window=3, #Contexto, distancia entre palabras predichas\n",
    "      ns_exponent=0.75, #Muestrea frecuencias por igual,exponente utilizado para dar forma a la distribución de muestreo negativa. Un valor de 1.0 muestrea exactamente en proporción a las frecuencias, 0.0 muestrea todas las palabras por igual, mientras que un valor negativo muestra las palabras de baja frecuencia más que las palabras de alta frecuencia. El popular valor predeterminado de 0,75 fue elegido por el documento original de Word2Vec.\n",
    "      dm_mean=1, #si es 0, use la suma de los vectores de palabras de contexto. Si es 1, usa la media.\n",
    "      #seed=25,\n",
    "      \n",
    "       )\n",
    "\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(ruta_drive+modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar un modelo ya entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x1f689694188>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruta_drive=\"./\"\n",
    "modelo=\"modelo_tweet\"\n",
    "\n",
    "model=Doc2Vec.load(ruta_drive+modelo)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_infer [ 0.04896088 -0.12312426 -0.11385608 -0.09553868  0.12385708]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "texto=\"fraude electoral\"\n",
    "v1 = model.infer_vector(tokenizar_stem(texto))\n",
    "print(\"V1_infer\", v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def inferir_df(modelo,df):\n",
    "    obs=df.tokens\n",
    "    ids=df.index\n",
    "    id_documento=[]\n",
    "    arrays=[]\n",
    "    i=0\n",
    "    while i<len(df):\n",
    "        try:\n",
    "            arrays.append(modelo.infer_vector(obs[i]))\n",
    "            id_documento.append(ids[i])\n",
    "            i=i+1\n",
    "        except:\n",
    "            arrays.append(modelo.docvecs[i])\n",
    "            id_documento.append(ids[i])\n",
    "            i=i+1\n",
    "\n",
    "    a=np.asarray(arrays)\n",
    "\n",
    "    df_doc2vec=pd.DataFrame(data=a[0:,0:],\n",
    "            index=id_documento,\n",
    "            columns=['v'+str(i) for i in range(a.shape[1])])\n",
    "    return(df_doc2vec)\n",
    "\n",
    "df_vector=inferir_df(model,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v0</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.548184</td>\n",
       "      <td>-0.220054</td>\n",
       "      <td>-0.920469</td>\n",
       "      <td>-0.634990</td>\n",
       "      <td>0.369438</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.277436</td>\n",
       "      <td>-0.115969</td>\n",
       "      <td>-0.618077</td>\n",
       "      <td>-0.313527</td>\n",
       "      <td>0.173461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.488431</td>\n",
       "      <td>-0.213024</td>\n",
       "      <td>-1.014365</td>\n",
       "      <td>-0.442136</td>\n",
       "      <td>0.232209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650157</td>\n",
       "      <td>-0.470660</td>\n",
       "      <td>-1.607086</td>\n",
       "      <td>-1.126487</td>\n",
       "      <td>0.370752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062106</td>\n",
       "      <td>-0.151343</td>\n",
       "      <td>-0.061181</td>\n",
       "      <td>-0.245937</td>\n",
       "      <td>-0.098158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         v0        v1        v2        v3        v4  clase\n",
       "0  0.548184 -0.220054 -0.920469 -0.634990  0.369438      0\n",
       "1  0.277436 -0.115969 -0.618077 -0.313527  0.173461      0\n",
       "2  0.488431 -0.213024 -1.014365 -0.442136  0.232209      0\n",
       "3  0.650157 -0.470660 -1.607086 -1.126487  0.370752      0\n",
       "4  0.062106 -0.151343 -0.061181 -0.245937 -0.098158      0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vector['clase']=df.clase\n",
    "#df_vector.to_csv(\"/content/drive/My Drive/NLP TALLER/tweet_sentimientos_vectores.csv\",sep=\"|\")\n",
    "df_vector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de un modelo de Aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separar 90% entrenamiento y 10% prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = df_vector['clase']\n",
    "X = df_vector.drop(labels=['clase'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y, \n",
    "                                                    test_size = 0.1, \n",
    "                                                    random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto Entrenamiento 11880 \n",
      "Conjunto Pruebas 1321 .\n",
      "negativos Entrenamiento   7027\n",
      "positivos Entrenamiento   4853\n",
      "negativos Test   781\n",
      "positivos Test   540\n"
     ]
    }
   ],
   "source": [
    "# Show the results of the split\n",
    "print(\"Conjunto Entrenamiento {} \".format(X_train.shape[0]))\n",
    "print(\"Conjunto Pruebas {} .\".format(X_test.shape[0]))\n",
    "\n",
    "print(\"negativos Entrenamiento  \",y_train[y_train==0].count())\n",
    "print(\"positivos Entrenamiento  \",y_train[y_train==1].count())\n",
    "\n",
    "print(\"negativos Test  \",y_test[y_test==0].count())\n",
    "print(\"positivos Test  \",y_test[y_test==1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score,precision_score, balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def _score_func(estimator, X, y):\n",
    "  y_pred_test = estimator.predict(X)\n",
    "  return balanced_accuracy_score(y, y_pred_test)\n",
    "\n",
    "\n",
    "class Class_Fit(object):\n",
    "    def __init__(self, clf, params=None):\n",
    "        if params:            \n",
    "            self.clf = clf(**params)\n",
    "        else:\n",
    "            self.clf = clf()\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def grid_search(self, parameters, Kfold):\n",
    "        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold,scoring=_score_func)\n",
    "        \n",
    "    def grid_fit(self, X, Y):\n",
    "        self.grid.fit(X, Y)\n",
    "        \n",
    "    def grid_predict(self, X, Y):\n",
    "        self.predictions = self.grid.predict(X)\n",
    "        print(\"Precision: {:.2f} % \".format(100*metrics.balanced_accuracy_score(Y, self.predictions)))\n",
    "\n",
    "# Este proceso tarda mucho, fue necesario hacerlo en una maquina propia\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Se utiliza gridsearch como modelo de selección para determinar los mejores parametros en XGBoost\n",
    "gb = Class_Fit(clf = XGBClassifier)\n",
    "param_grid = {\n",
    "             'objective':['binary:logistic'],\n",
    "             'n_estimators' : [100],\n",
    "              'eta':[0.0003,0.003],\n",
    "              \n",
    "            #  'gamma':[0.03,0.003,5],\n",
    "              'max_depth':[10,20],\n",
    "              'min_child_weight':[0.5,5,0],\n",
    "      #        'max_delta_step':[0,5,10],\n",
    "           #   'subsample':[0.5,1],\n",
    "          #    'sampling_method':['subsample','uniform','gradient_based'],\n",
    "       #       'colsample_bytree': [0.2,1],\n",
    "           #   'lambda':[1,0],\n",
    "              'alpha':[0.003,0.0003],\n",
    "              'scale_pos_weight':[y_train[y_train==0].count()/y_train[y_train==1].count(),0.5],\n",
    "              'n_job':[-1],\n",
    "              'booster':['gblinear','gbtree'],  \n",
    "             }\n",
    "gb.grid_search(parameters = param_grid, Kfold = 5)\n",
    "gb.grid_fit(X = X_train, Y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.003,\n",
       " 'booster': 'gbtree',\n",
       " 'eta': 0.003,\n",
       " 'max_depth': 20,\n",
       " 'min_child_weight': 0.5,\n",
       " 'n_estimators': 100,\n",
       " 'n_job': -1,\n",
       " 'objective': 'binary:logistic',\n",
       " 'scale_pos_weight': 1.4479703276323923}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.003, base_score=0.5, booster='gbtree',\n",
       "       colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "       eta=0.003, gamma=0, gpu_id=-1, importance_type='gain',\n",
       "       interaction_constraints=None, learning_rate=0.00300000003,\n",
       "       max_delta_step=0, max_depth=20, min_child_weight=0.5, missing=nan,\n",
       "       monotone_constraints=None, n_estimators=100, n_job=-1, n_jobs=0,\n",
       "       num_parallel_tree=1, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0.00300000003, reg_lambda=1,\n",
       "       scale_pos_weight=1.4479703276323923, subsample=1, tree_method=None,\n",
       "       validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score,precision_score, balanced_accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "params={'alpha': 0.003,\n",
    " 'booster': 'gbtree',\n",
    " 'eta': 0.003,\n",
    " 'max_depth': 20,\n",
    " 'min_child_weight': 0.5,\n",
    " 'n_estimators': 100,\n",
    " 'n_job': -1,\n",
    " 'objective': 'binary:logistic',\n",
    " 'scale_pos_weight': 1.4479703276323923}\n",
    "\n",
    "modelo_aprendizaje = XGBClassifier(**params)\n",
    "modelo_aprendizaje.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[562, 219],\n",
       "       [248, 292]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Train:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[6728,  299],\n",
       "       [ 688, 4165]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy test: 0.6464799394398183\n",
      "accuracy train: 0.9169191919191919\n",
      "balanced_accuracy test: 0.6301655048133921\n",
      "balanced_accuracy train: 0.9078409288877838\n"
     ]
    }
   ],
   "source": [
    "def eval_modelo(X_test,X_train,modelo):\n",
    "    #Predice prueba\n",
    "    y_pred_test = modelo.predict(X_test)\n",
    "    #predice train\n",
    "    y_pred_train = modelo.predict(X_train)\n",
    "\n",
    "    #n_samples / (n_classes * np.bincount(y))\n",
    "    display('Test:',confusion_matrix(y_test, y_pred_test))\n",
    "    display('Train:',confusion_matrix(y_train, y_pred_train))\n",
    "\n",
    "    print(\"accuracy test:\",accuracy_score(y_test, y_pred_test))\n",
    "    print(\"accuracy train:\",accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "    print(\"balanced_accuracy test:\",balanced_accuracy_score(y_test, y_pred_test))\n",
    "    print(\"balanced_accuracy train:\",balanced_accuracy_score(y_train, y_pred_train))\n",
    "\n",
    "eval_modelo(X_test,X_train,modelo_aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./modelo_clasificacion_tweets.dat']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load # Guardar/Cargar modelos\n",
    "modelo_aprendizaje = XGBClassifier(**params)\n",
    "modelo_aprendizaje.fit(X, Y)\n",
    "dump(modelo_aprendizaje,\"./modelo_clasificacion_tweets.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comentario: gobierno opresor\n",
      "probabilidad negativo: 0.5708834\n",
      "probabilidad positivo: 0.42911658\n",
      "clase: [0]\n"
     ]
    }
   ],
   "source": [
    "def encode_d(document: str) -> torch.Tensor:\n",
    "    return model.infer_vector(tokenizar_stem(document))\n",
    "\n",
    "def clasificar(documento,modelo):\n",
    "    x=encode_d(documento)\n",
    "    x_pred=pd.DataFrame(x,X.columns).transpose()\n",
    "    print(\"comentario:\",documento)\n",
    "    print(\"probabilidad negativo:\",modelo_aprendizaje.predict_proba(x_pred)[0][0])\n",
    "    print(\"probabilidad positivo:\",modelo_aprendizaje.predict_proba(x_pred)[0][1])\n",
    "    print(\"clase:\",modelo_aprendizaje.predict(x_pred))\n",
    "    \n",
    "\n",
    "#modelo_aprendizaje=load(\"/content/drive/My Drive/NLP TALLER/modelo_clasificacion_tweets.dat\")\n",
    "#documento='estoy feliz y contento de estar aquí en clases que alegría'\n",
    "documento='gobierno opresor'\n",
    "clasificar(documento,modelo_aprendizaje)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
